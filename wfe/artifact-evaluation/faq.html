<br>	
<center><h3>Frequently Asked Questions</h3>

<small><i>This page continuously evolves and can be modified directly at <a href="https://github.com/ctuning/ck-artifact-evaluation/blob/master/wfe/artifact-evaluation/faq.html">GitHub</a></i></small>!
</center>

<br>
<p>
<b>If you have questions or suggestions which are not addressed here, please feel free 
to post them to the dedicated <a href="https://groups.google.com/forum/#!forum/artifact-evaluation">AE google group</a>.</b>

<!----------------------------------------------------------------------------------------------------->
<h3>Do I have to open source my software artifacts?</h3>

No, it is not strictly necessary and you can 
provide your software artifact as a binary.
However, in case of problems, reviewers may not be 
able to fix it and will likely give you a negative score.

<!----------------------------------------------------------------------------------------------------->
<h3>Is Artifact evaluation blind or double-blind?</h3>

AE is a single-blind process, i.e. authors' names are known to the evaluators
(there is no need to hide them since papers are accepted),
but names of evaluators are not known to authors.
AE chairs are usually used as a proxy between authors and evaluators
in case of questions and problems.

<!----------------------------------------------------------------------------------------------------->
<h3>How to pack artifacts?</h3>

We do not have strict requirements at this stage. You can pack 
your artifacts simply in a tar ball, zip file, Virtual Machine or Docker image.
You can also share artifacts via public services including GitHub, GitLab and BitBucket.

Please see <a href="$#ck_root_page_url#$submission$#ck_page_suffix#$">our submission guide</a> 
for more details.

<!----------------------------------------------------------------------------------------------------->
<h3>Is it possible to provide a remote access to a machine with pre-installed artifacts?</h3>

Only in exceptional cases, i.e. when rare hardware or proprietary software/benchmarks are required,
or VM image is too large or when you are not authorized to move artifacts outside your organization.
In such case, you will need to send the access information 
to the AE chairs via private email or SMS. 
They will then pass this information to the evaluators.

<!----------------------------------------------------------------------------------------------------->
<h3>Can I share commercial benchmarks or software with evaluators?</h3>

Please check the license of your benchmarks, data sets and software. 
In case of any doubts, try to find a free alternative. In fact, 
we strongly suggest you provide a small subset of free benchmarks 
and data sets to simplify the evaluation process.

<!----------------------------------------------------------------------------------------------------->
<h3>Can I engage with the community to evaluate my artifacts?</h3>

<p>
Based on the community feedback, we allow open evaluation
to let the community validate artifacts which are publicly available 
at GitHub, GitLab, BitBuckets, etc, report issues and help the authors 
to fix them. 

Note, that in the end, these artifacts still go through traditional
evaluation process via the AE committee. We successfully validated 
at <a href="http://adapt-workshop.org/motivation2016.html">ADAPT'16</a> 
and CGO/PPoPP'17!

<!----------------------------------------------------------------------------------------------------->
<h3>How to automate, customize and port experiments?</h3>

From our <a href="https://www.reddit.com/r/MachineLearning/comments/ioq8do/n_reproducing_150_research_papers_the_problems">past experience reproducing research papers</a>, 
the major difficulty that evaluators face is the lack of a common and portable workflow framework
in ML and systems research. This means that each year they have 
to learn some ad-hoc scripts and formats in nearly 
all artifacts without even reusing such knowledge the following year.

Things get even worse if an evaluator would like to validate experiments 
using a different compiler, tool, library, data set, operating systems or hardware
rather than just reproducing quickly outdated results using 
VM and Docker images - our experience shows that most of the submitted scripts 
are not easy to change, customize or adapt to other platform.

<p>
That is why we collaborate with <a href="https://cKnowledge.org/partners.html">the community</a> 
and <a href="https://acm.org">ACM</a> to develop a <a href="https://github.com/ctuning/ck">common experimental framework (CK)</a>.

You can see how CK workflows helped to automate, crowdsource and visualize experiments in the 
<a href="https://cKnowledge.org/request">1st ACM ReQuEST-ASPLOS'18 tournament</a> 
to co-design Pareto-efficient software/hardware stack for deep learning:
 <a href="https://github.com/ctuning/ck-request-asplos18-results">CK workflows</a>,
   <a href="https://doi.org/10.1145/3229762">ACM proceedings</a>,
   <a href="https://portalparts.acm.org/3230000/3229762/fm/frontmatter.pdf">report</a> and
   <a href="https://cKnowledge.io/reproduced-results">public dashboards with reproducible results</a>.

You can find reproduced papers with portable CK workflows and reusable components
using the <a href="https://cknowledge.io/?q=%22reproduced-papers%22%20AND%20%22portable-workflow-ck%22">cKnowledge.io portal</a>.

Please, follow <a href="https://ck.readthedocs.io/en/latest/src/typical-usage.html">this guide</a> 
if you want to convert your artifacts and workflows to the CK format.

<!----------------------------------------------------------------------------------------------------->
<h3>Do I have to make my artifacts public if they pass evaluation?</h3>

No, you don't have to and it may be impossible in the case of commercial artifacts.
Nevertheless, we encourage you to make your artifacts publicly available upon publication, 
for example, by including them in a permanent repository (required to receive the "artifact available" badge)
to support open science as outlined in <a href="http://dl.acm.org/citation.cfm?id=2618142">our vision</a>.

<p>
Furthermore, if you make  your artifacts publicly available at the time
of submission, you may profit from the "public review" option, where you are engaged
with the community to discuss, evaluate and use your software. See such
examples <a href="https://cTuning.org/ae/artifacts.html">here</a> 
(search for "public evaluation").

<!----------------------------------------------------------------------------------------------------->
<h3>How to report and compare empirical results?</h3>

<b>News:</b> Please check the <a href="https://www.sigarch.org/a-checklist-manifesto-for-empirical-evaluation-a-preemptive-strike-against-a-replication-crisis-in-computer-science/">SIGARCH empirical checklist</a>
and the <a href="https://www.cs.mcgill.ca/~jpineau/ReproducibilityChecklist.pdf">NeurIPS reproducibility checklist</a>.

<br><br>
First of all, you should undoubtedly run empirical experiments more than once 
(we still encounter many cases where researchers measure execution time only once).
and perform statistical analysis.

There is no universal recipe how many times you should repeat your empirical experiment 
since it heavily depends on the type of your experiments, platform and environment. 
You should then analyze the distribution of execution times as shown in the figure below:

<center><img src="https://raw.githubusercontent.com/ctuning/ck-assets/master/slide/reproducibility/994e7359d7760ab1-cropped.png"></center>

<p>If you have more than one expected value (b), it means that you have several
run-time states in your system (such as adaptive frequency scaling) 
and you can not use average and reliably compare empirical results.

However, if there is only one expected value for a given experiment (a), 
then you can use it to compare multiple experiments. This is particularly
useful when running experiments across different platforms from different
users as described in this <a href="https://cKnowledge.org/rpi-crowd-tuning">article</a>.
 
<p>
You should also report the variation of empirical results together with all expected values.
Furthermore, we strongly suggest you to pre-record results from your platform
and provide a script to automatically compare new results with the pre-recorded ones.
Otherwise, evaluators can spend considerable amount of time 
digging out and validating results from "stdout".
For example, see how new results are visualized and compared against the pre-recorded ones
using <a href="https://github.com/SamAinsworth/reproduce-cgo2017-paper/files/618737/ck-aarch64-dashboard.pdf">some dashboard</a> 
in the <a href="https://github.com/SamAinsworth/reproduce-cgo2017-paper">CGO'17 artifact</a>.



<!----------------------------------------------------------------------------------------------------->
<h3>How to deal with numerical accuracy and instability?</h3>

If the accuracy of your results depends on a given machine, environment and optimizations 
(for example, when optimizing BLAS, DNN, etc), you should provide a script to automatically 
report unexpected loss in accuracy above provided threshold as well as any numerical instability.

<!----------------------------------------------------------------------------------------------------->
<h3>How to validate models or algorithm scalability?</h3>

If you present a novel parallel algorithm or some predictive model which should scale 
across a number of cores/processors/nodes, we suggest you 
to provide an experimental workflow that could automatically detect the topology 
of a user machine, validate your models or algorithm scalability, 
and report any unexpected behavior. 

<!----------------------------------------------------------------------------------------------------->
<h3>Is there any page limit for my Artifact Evaluation Appendix?</h3>

There is no limit for the AE Appendix at the time of the submission for Artifact Evaluation.

<p>
However, there is currently a 2 page limit for the AE Appendix in the camera-ready CGO, PPoPP, ASPLOS and MLSys papers.
There is no page limit for the AE Appendix in the camera-ready SC paper. We also expect 
that there will be no page limits for AE Appendices in the journals willing to participate 
in the AE initiative.

<!----------------------------------------------------------------------------------------------------->
<a name="hotcrp"><h3>Where can I find a sample HotCRP configuration to set up AE?</h3>

Please, check out our <a href="https://www.linkedin.com/pulse/acm-ppopp19-artifact-evaluation-report-hotcrp-grigori-fursin/">PPoPP'19 HotCRP configuration for AE</a> in case you need to set up your own HotCRP instance.

<!----------------------------------------------------------------------------------------------------->
<a name="artifacts_for_acm_dl"><h3>How to prepare my artifact for the ACM DL?</h3>

<b>News:</b> ACM is not accepting artifacts in the DL at this moment. Therefore we suggest you to use Zenodo or FigShare to deposit your artifacts.

<br><br>

The procedure is not yet automated and our ACM colleagues are working on that. 

In the meantime, you need to prepare at least three files 
and make them available to your Artifact Evaluation chairs
(for example via Dropbox or Google Drive):

<ol>
 <li>Main archive of your artifact in zip, tar, VM/Docker image or other formats (for example, <i>artifact.zip</i>).
     <br>
     <br>

 <li>PDF of your Artifact Appendix with the final title and authors of your accepted paper (for example, <i>artifact-appendix.pdf</i>). Please use the same template and formatting as in your camera-ready paper.
     <br>
     <br>

 <li>An XML description of your artifact (for example, <i>artifact.xml</i>) - this is the most tricky part since it has to be done manually at the moment (we expect that ACM will add a web front-end in the ACM DL to prepare such descriptions).

     <p>
     Please, select one XML artifact template from below and update all fields with your own information:
     <ul>
      <li><a href="https://github.com/ctuning/ck-artifact-evaluation/blob/master/xml/acm-artifact-dtd-1.1/example_acm_artifact_description_ppopp.xml">XML template for generic artifacts without any specific format</a>
      <li><a href="https://github.com/ctuning/ck-artifact-evaluation/blob/master/xml/acm-artifact-dtd-1.1/example_acm_artifact_description_request.xml">XML template for artifacts using Collective Knowledge workflow framework (based on ACM ReQuEST tournament)</a>
     </ul>

     <p>
     You can skip updating the following fields which will be later updated by ACM:
     <ul>
      <li>artifact_publication_date
      <li>publisher
      <li>article_doi
     </ul>

     <p>
     You can generate CSS (ACM Computing Classification System) for your artifact
     using this <a href="https://dl.acm.org/ccs/ccs_flat.cfm">online ACM tool</a>.

     <p>
     Don't forget to provide correct names for your files (archive and appendix) in this XML.

     <p>
     <b>Note, that you have to validate the correctness of your XML against the following
        <a href="https://github.com/ctuning/ck-artifact-evaluation/blob/master/xml/acm-artifact-dtd-1.1/acm_artifact_v1.1.dtd">ACM DTD file</a></b>.

        If you don't know how to do it, we provided a reusable <a href="https://ReproIndex.com/actions">CK action (CK API)</a> for such validation. You can validate your XML as follows:
        <ol>
         <li>Install CK as described <a href="https://github.com/ctuning/ck#installation">here</a> (Linux, Windows, MacOS).<BR><BR>
         <li>If you already have some CK repositories installed, update them as follows: <pre>ck pull all</pre>
         <li>Install the Artifact Evaluation repository as follows: <pre>ck pull repo:ck-artifact-evaluation</pre>
         <li>Validate your XML as follows until there are not mistakes: <pre>ck validate xml:acm-artifact-dtd-1.1 --xml_file=artifact.xml</pre>
        </ol>

        <p>
        The code for the above CK actions is available at GitHub: 
        <a href="https://github.com/ctuning/ck-env/blob/master/module/xml/module.py">XML actions</a>,
        <a href="https://github.com/ctuning/ck-artifact-evaluation/blob/master/module/ae/module.py">AE actions</a>.
</ol>

<!--If needed, you can add more files if you think that they should be separated from the main archive. However, you then need to describe them in the XML of your artifact.-->

<p>
Please, see an example of a paper and its artifact in the ACM Digital library (check "source materials" tab):
<ul>
 <li><a href="https://dl.acm.org/citation.cfm?id=3229763">Paper "Highly Efficient 8-bit Low Precision Inference of Convolutional Neural Networks with IntelCaffe"</a>
 <li><a href="https://github.com/ctuning/ck-request-asplos18-caffe-intel">Artifact for the above paper</a>
</ul>

<!----------------------------------------------------------------------------------------------------->
<a name="contacts"><h3>Questions and Feedback</h3>

If you have any questions, do not hesitate to get in touch with the AE community 
using this <a href="https://groups.google.com/forum/#!forum/artifact-evaluation">public discussion group</a>!

<br>
<br>
