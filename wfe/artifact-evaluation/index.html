<p>
<h3>News</h3>

<img src="$#ck_url_template_pull#$resources/logo-help3.png" align="right">

<ul>

 <li><b>2022 December:</b> We will be preparing Artifact Evaluation at ACM/IEEE MICRO'23 
        and will help with Student Cluster Competition at SuperComputing'23 -
        please stay tuned for more details or <a href="https://cknow.io/@gfursin">get in touch</a> 
        if you are interested to participate and/or help!
 
 <li><b>2022 November:</b> We have prepared a <a href="https://github.com/mlcommons/ck/blob/master/docs/tutorials/sc22-scc-mlperf.md">tutorial</a> 
        for the <a href ="https://studentclustercompetition.us/2022/index.html">Student Cluster Competition</a> to run a modular version of the 
        MLPerf inferenece benchmark using the <a href="https://github.com/mlcommons/ck">MLCommons CM workflow automation meta-framework</a>. 
        Feel free to try it - you should be able to reproduce it within 15 minutes!

 <li><b>2022 October:</b> We kickstarted an open MLCommons workgroup on education and reproducibility - 
        everyone is welcome to join <a href="https://github.com/mlcommons/ck/blob/master/docs/mlperf-education-workgroup.md">here</a>.

 <li><b>2022 September:</b> We have helped MLCommons to prepare and release <a href="https://github.com/mlcommons/ck/releases/tag/cm-v1.0.1">CM v1.0.1</a> - 
        the next generation of the MLCommons Collective Knowledge framework being developed 
        by the <a href="https://github.com/mlcommons/ck/blob/master/docs/mlperf-education-workgroup.md">public workgroup</a>
        to support collaborative and reproducible ML & Systems research!
        We are very glad to see that more than 80% of all performance results and more than 95% of all power results 
        were automated by the MLCommons CK v2.6.1 in the <a href="https://mlcommons.org/en/news/mlperf-inference-v21/">latest MLPerf inference round</a> 
        thanks to submissions from Qualcomm, Krai, Dell, HPE and Lenovo!

 <li>
  2022.April:> We are developing the 2nd version of the CK framework 
     to make it easier to transfer scientific knowledge to production systems: 
  <a href="https://github.com/mlcommons/ck/tree/master/cm">GitHub</a>.
 </li>

 <li>
  2022.March: We've successfully completed Artifact Evaluation at <a href="$#ck_root_page_url#$asplos2022$#ck_page_suffix#$">ASPLOS 2022</a>.
 </li>


 <li>
  <a href="https://learning.acm.org/techtalks/reproducibility">ACM TechTalk (video)</a> about artifact evaluation (challenges and solutions).
 </li>

 <li>
  Artifact Evaluation at <a href="https://www.microarch.org/micro54/submit/artifacts.php?s=09">MICRO 2021</a>.
 </li>

 <li>
  The report from the "Workflows Community Summit: Bringing the Scientific Workflows Community Together" 
  is available in <a href="https://arxiv.org/abs/2103.09181">ArXiv</a>.
 </li>

 <li>
  Artifact Evaluation: reproducing papers at <a href="https://cTuning.org/ae/asplos2021.html">ASPLOS 2021</a> 
  (<a href="https://cknow.io/?q=%22papers-asplos-2021%22">the list of accepted artifacts</a>).
 </li>

 <li>The paper about automating artifact evaluation has appeared in the Philosophical Transactions A, 
     the world's longest-running journal where Newton published: <a href="https://doi.org/10.1098/rsta.2020.0211">DOI</a>, 
     <a href="https://arxiv.org/pdf/2011.01149.pdf">ArXiv</a>.
 </li>

 <li>
  <a href="https://cTuning.org">cTuning foundation</a> is honored to join MLCommons 
  as a founding member to accelerate machine learning innovation and 
  help with best practices, reproducible benchmarking and workflow automation
  along with 50+ leading companies and universities: 
  <a href="https://mlcommons.org/en/news/mlcommons-launch">press release</a>.
 </li>

 <li>
  <a href="https://www.reddit.com/r/MachineLearning/comments/ioq8do/n_reproducing_150_research_papers_the_problems">Reddit discussion</a> about reproducing ML and systems papers.
 </li>

 <li>
  Artifact Evaluation: reproducing papers at <a href="https://cknow.io/event/repro-mlsys2020">MLSys 2020</a>
  (<a href="https://cknow.io/?q=%22papers-mlsys-2020%22">the list of accepted artifacts</a>).
 </li>

 <li>
  Artifact Evaluation: reproducing papers at <a href="https://cknow.io/event/repro-asplos2020">ACM ASPLOS 2020</a>
  (<a href="https://cknow.io/?q=%22papers-asplos-2020%22">the list of accepted artifacts</a>).
 </li>

 <li>
  Building an open repository with reproduced papers, portable workflows and reusable artifacts: <a href="https://cknow.io">cknow.io</a>.
 </li>

 <li>
  Working on a common methodology to share research artifacts (code, data, models) at systems and ML conferences: <a href="$#ck_root_page_url#$checklist$#ck_page_suffix#$">ACM/cTuning</a>.
 </li>

 <p>
 <li>
  <i><a href="https://cknow.io/papers">All reproduced papers</a></i>.
 </li>
 <li>
  <i><a href="https://cknow.io/events">All our prior reproducibility initiatives with shared artifacts</a></i>.
 </li>
</ul>


<!------------------------------------------------>
<p>
<h3><a name="motivation">Motivation</h3>

<ul>

<p>
Researchers <a href="https://arxiv.org/pdf/2011.01149.pdf">struggle</a> 
to reproduce experimental results and reuse research code from scientific papers
due to continuously changing software and hardware, lack of common APIs, 
stochastic behavior of computer systems and a lack of a common experimental methodology.
That is why we decided to <a href="https://cknow.io/events">help systems and ML conferences</a>
validate results from accepted papers with the help of   
<a href="$#ck_root_page_url#$committee$#ck_page_suffix#$">independent evaluators</a> 
while collaborating with ACM on a common methodology, reproducibility checklist and tools 
to <a href="https://doi.org/10.5281/zenodo.4005773">automate this tedious process</a>. Papers that successfully pass such evaluation process 
receive a set of <a href="https://www.acm.org/publications/policies/artifact-review-badging">ACM reproducibility badges</a> printed on the papers themselves:

 <center>
  <img src="https://www.acm.org/binaries/content/gallery/acm/publications/replication-badges/artifacts_available_dl.jpg" width="64">
  <img src="https://www.acm.org/binaries/content/gallery/acm/publications/replication-badges/artifacts_evaluated_functional_dl.jpg" width="64">
  <img src="https://www.acm.org/binaries/content/gallery/acm/publications/replication-badges/artifacts_evaluated_reusable_dl.jpg" width="64">
  <img src="https://www.acm.org/binaries/content/gallery/acm/publications/replication-badges/results_replicated_dl.jpg" width="64">
  <img src="https://www.acm.org/binaries/content/gallery/acm/publications/replication-badges/results_reproduced_dl.jpg" width="64">
 </center>

<br>
<p>
Please check our <a href="$#ck_root_page_url#$submission_extra$#ck_page_suffix#$">"submission"<a> 
and <a href="$#ck_root_page_url#$reviewing$#ck_page_suffix#$">"reviewing"</a> 
guidelines for more details.
 If you have questions or suggestions, 
 do not hesitate to participate in our public discussions 
 using this <a href="https://groups.google.com/forum/#!forum/artifact-evaluation">Artifact Evaluation google group</a>
 or the <a href="https://www.linkedin.com/groups/7433414">LinkedIn group</a>.

</ul>

<center>
<img src="$#ck_url_template_pull#$resources/cknowledge-platform.png" width="50%"><br>
</center>

<!------------------------------------------------>
<p>
<h3>Related initiatives:</h3>

<ul>
 <li>
  <a href="https://www.niso.org/publications/rp-31-2021-badging">NISO RP-31-2021, Reproducibility Badging and Definitions</a>
 </li>
 
 <li>
  An <a href="https://cknow.io/browse">Open knowledge portal about complex compuational systems (AI, ML, quantum, IoT)</a> with <a href="https://cknow.io/results">public SOTA scoreboards and reproduced papers</a>;
 </li>

 <li><a href="https://www.sigarch.org/a-checklist-manifesto-for-empirical-evaluation-a-preemptive-strike-against-a-replication-crisis-in-computer-science/">SIGPLAN's checklist for empirical evaluation</a>;

 <li><a href="https://www.cs.mcgill.ca/~jpineau/ReproducibilityChecklist.pdf">The Machine Learning Reproducibility Checklist</a> (NeurIPS);

 <li>
  The NASEM report on Reproducibility and Replicability in Science: <a href="https://www.nap.edu/catalog/25303/reproducibility-and-replicability-in-science">Online PDF</a> 
  and a <a href="https://sc19.supercomputing.org/session/?sess=sess293">BoF at SC19</a> organized and chaired by <a href="https://lorenabarba.com">Lorena A. Barba</a>;
 </li>

 <li>
  <a href="https://docs.google.com/document/d/1pqzPtLVIvwLwJsZwCb2r7yzWMaifudHe1Xvn42T4CcA/edit">HOWTO for AEC Submitters</a> (Dan Borowy, Charlie Cursinger, Emma Tosch, John Vilk, and Emery Berger)
 </li>


 <li>
   A <a href="https://github.com/ctuning/ck-scc18/wiki">Portable workflow</a> to automate the SeiSol application from the Supercomputing'18 Student Cluster Competition reproducibility challenge
    and the <a href="https://github.com/reproindex/ck-scc">SCC automation workflow</a>;
 </li>

 <li>
  <a href="https://www.linkedin.com/pulse/acm-ppopp19-artifact-evaluation-report-hotcrp-grigori-fursin/">PPoPP'19 HotCRP configuration for artifact evaluation</a>;
 </li>

 <li>
  Reproducible optimization tournaments on <a href="https://cKnowlege.org/request">AI/ML/SW/HW co-design</a>
  and on <a href="https://cKnowlege.org/quantum">quantum computing challenges</a>;
 </li>

 <li>
  <a href="https://groups.google.com/forum/#!forum/artifact-evaluation">Artifact Evaluation google group</a>.
 </li>

</ul>




<!------------------------------------------------>
<p>
<h3>Sponsors and supporters</h3>

 <ul>
   <div id="ck_entries_space8"></div>

   <a href="https://acm.org"><img src="$#ck_url_template_pull#$resources/logo_acm_small.png"></a>
   &nbsp;&nbsp;&nbsp;&nbsp;
   <a href="https://cTuning.org"><img src="$#ck_url_template_pull#$resources/ctuning-logo1.png"></a>
 </ul>

<br>