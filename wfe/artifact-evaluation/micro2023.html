<center>
 <p>
  <span style="font-size:30px;"><b><span style="font-family: tahoma,geneva,sans-serif">
  Artifact Evaluation for MICRO 2023
  </span></b></span>
 </p>

<!------------------------------------------------>
 <span style="font-size:small">
  [ <a href="https://microarch.org/micro56">Back to the ACM/IEEE MICRO 2023 conference website</a> ]
 </span>
</center>


<br>


<!------------------------------------------------>
<h4>Important dates</h4>

<div style="margin-left:20px;">
 Paper rebuttal/Revision: <i>June 26 – July 7, 2023</i><br>
 Paper decision: <i>July 24, 2023</i><br>
 Artifact submission: <i>TBA</i><br>
 Artifact decision: <i>TBA</i><br>
 Conference: <a href="https://microarch.org/micro56/">October 28 - November 1, 2023</a> (Toronto, Canada)<br>
</div>

<!------------------------------------------------>
<div id="vspace8"></div>
<div id="vspace8"></div>
<h4>Artifact evaluation chairs</h4>

<div style="margin-left:20px;">
 <ul>
  <li><a href="https://people.csail.mit.edu/suvinay/about.html">Suvinay Subramanian</a> (Google)<br>
  <li><a href="https://www.linkedin.com/in/karthik-murthy-b1b1314">Karthik Murthy (Google)<br>
  <li><a href="https://cKnowledge.org/gfursin">Grigori Fursin</a> (cTuning foundation, cKnowledge Ltd, MLCommons)<br>
 </ul>
</div>



<!------------------------------------------------>
<div id="vspace8"></div>
<div id="vspace8"></div>
<h4>Artifact evaluation process</h4>

<div style="margin-left:20px;">

<p>
 Artifact evaluation promotes reproducibility of experimental results and
 encourages code and data sharing to help the community quickly validate and
 compare alternative approaches. Authors of accepted papers are invited to
 formally describe supporting materials (code, data, models, workflows, results)
 using the
 <a href="https://github.com/mlcommons/ck/blob/master/docs/artifact-evaluation/template/ae.tex">standard Artifact Appendix template</a>
 and submit it with the paper and materials for evaluation.

<div id="vspace4"></div>
<p>
 This submission is <b>voluntary and will not influence</b> the final
 decision regarding the papers. We want to help the authors validate
 experimental results from their accepted papers by an independent AE Committee
 in a collaborative way while helping readers find articles with available,
 functional, and validated artifacts!



<p>
 The papers that successfully go through AE will receive a set of
 <a href="https://www.acm.org/publications/policies/artifact-review-and-badging-current">ACM badges</a>
 of approval printed on the papers themselves and available as meta information
 in the ACM Digital Library (it is now possible to search for papers with
 specific badges in ACM DL). Authors of such papers will have an option to
 include up to 2 pages of their Artifact Appendix to the camera-ready paper.

  <table border="0">
   <tr>
    <td>
     <img src="https://www.acm.org/binaries/content/gallery/acm/publications/replication-badges/artifacts_available_dl.jpg" height="64">
    </td>
    <td>
     Artifact available
    </td>
   </tr>
   <tr>
    <td>
     <img src="https://www.acm.org/binaries/content/gallery/acm/publications/replication-badges/artifacts_evaluated_functional_dl.jpg" height="64">
    </td>
    <td>
     Artifact evaluated - functional
    </td>
   </tr>
   <tr>
    <td>
     <img src="https://www.acm.org/binaries/content/gallery/acm/publications/replication-badges/results_reproduced_dl.jpg" height="64">
    </td>
    <td>
     Results reproduced
    </td>
   </tr>
  </table>


<p>
<i>
Since criteria for the ACM "Artifacts Evaluated – Reusable" badge are quite vague, we partnered with the 
<a href="https://github.com/mlcommons/ck/blob/master/docs/taskforce.md">MLCommons task force on automation and reproducibility</a> 
to add their <a href="https://github.com/mlcommons/ck/blob/master/docs/README.md">unified interface</a> 
to the submitted artifacts to make them more portable, reproducible and reusable. 
It is optional and we will test it as a possible criteria to obtain the ACM reusability badge. 
Our long-term goal is to make it easier to evaluate and reuse artifacts across diverse and rapidly evolving software and hardware. 
We suggest the authors to join the <a href="https://discord.gg/JjWNWXKxwT">public Discord server for this task force</a> 
to get free help from the community and MLCommons
to add this interface to their artifacts before evaluation. The authors can also try to add this unified interface themselves 
following <a href="https://github.com/mlcommons/ck/blob/master/docs/tutorials/common-interface-to-reproduce-research-projects.md">this tutorial</a>.
Artifacts with a unified interface will receive an extra ACM "Artifacts Evaluated – Reusable" badge:<br>

  <table border="0">
   <tr>
    <td>
     <img src="https://www.acm.org/binaries/content/gallery/acm/publications/replication-badges/artifacts_evaluated_reusable_dl.jpg" height="64">.
    </td>
    <td>
     Artifact reusable
    </td>
   </tr>
  </table>

</i>

</div>


<!------------------------------------------------>
<div id="vspace8"></div>
<div id="vspace8"></div>
<h4>Artifact preparation</h4>

<div style="margin-left:20px;">

<p>
 You need to prepare the <a href="https://github.com/mlcommons/ck/blob/master/docs/artifact-evaluation/template/ae.tex">Artifact Appendix</a>
 describing all software, hardware and data set dependencies, key results to be reproduced, and how to prepare, run and validate experiments.

 We suggest the authors and evaluators to join our <a href="https://discord.gg/JjWNWXKxwT">public Discord server for automation and reproducibility</a> 
 to get free help from the community and MLCommons to prepare and submit your artifacts (#general and #artifact-evaluation channels).

 Though it is relatively intuitive and based on our
 <a href="$#ck_root_page_url#$prior_ae$#ck_page_suffix#$">past AE experience and your feedback</a>,
 we strongly encourage you to check the
  the <a href="https://github.com/mlcommons/ck/blob/master/docs/artifact-evaluation/checklist.md">Artifact Appendix guide</a>,
  <a href="https://github.com/mlcommons/ck/blob/master/docs/artifact-evaluation/reviewing.md">artifact reviewing guide</a>,
  the <a href="https://www.sigplan.org/Resources/EmpiricalEvaluation">SIGPLAN Empirical Evaluation Guidelines</a>,
  the <a href="https://www.cs.mcgill.ca/~jpineau/ReproducibilityChecklist.pdf">NeurIPS reproducibility checklist</a>
  and <a href="https://github.com/mlcommons/ck/blob/master/docs/artifact-evaluation/faq.md">AE FAQs</a> before submitting artifacts for evaluation!

 You can find the examples of Artifact Appendices
 in the following <a href="https://cknow.io/reproduced-papers">reproduced papers</a>.

 <p>
 <i>Since the AE methodology is slightly different at different conferences, we introduced the unified Artifact Appendix
 with the Reproducibility Checklist to help readers understand what was evaluated and how! Furthermore, artifact evaluation
 sometimes help to discover some minor mistakes in the accepted paper -
 in such case you have a chance to add related notes and corrections
 in the Artifact Appendix of your camera-ready paper!</i>

<p>
 We strongly recommend you to provide at least some scripts to build your workflow,
 all inputs to run your workflow, and some expected outputs to validate results from your paper.
 You can then describe the steps to evaluate your artifact
 using <a href="https://jupyter.org">Jupyter Notebooks</a>
 or plain README files.
 <b>You can skip this step if you want to share your artifacts without the validation of experimental results</b> -
 in such case your paper can still be entitled for the "artifact available" badge!

<p>
We encourage you to participate in our pilot project with the <a href="https://github.com/mlcommons/ck/blob/master/docs/taskforce.md">MLCommons task force on automation and reproducibility</a>
to provide a common interface for your artifacts in a non-intrusive way and thus make it easier to prepare, run, reproduce and reuse experiments with the help of the 
<a href="https://github.com/mlcommons/ck/blob/master/docs/README.md">MLCommons CM automation language</a>.
We will use it as one of criteria to give you the ACM "Artifacts Evaluated – Reusable" badge.
You will get free help from the community and MLCommons via <a href="https://discord.gg/JjWNWXKxwT">public Discord server</a> 
or you can try to add this interface yourself using <a href="https://github.com/mlcommons/ck/blob/master/docs/tutorials/common-interface-to-reproduce-research-projects.md">this tutorial</a>.

</div>

<!------------------------------------------------>
<div id="vspace8"></div>
<div id="vspace8"></div>
<h4>Artifact submission</h4>

<div style="margin-left:20px;">

<p>
 Submit the artifact abstract and the PDF of your paper with the Artifact
 Appendix attached using the <a href="https://micro23-ae.hotcrp.com/">AE submission website</a>.

<p>
 The (brief) abstract should describe your artifact, the minimal hardware and
 software requirements, how it supports your paper, how it can be validated and
 what the expected result is. Do not forget to specify if you use any
 proprietary software or hardware! This abstract will be used by evaluators
 during artifact bidding to make sure that they have an access to appropriate
 hardware and software and have required skills.

<p>
If you artifacts are already publicly available via GitHub or GitLab, 
you can choose an "open evaluation" option to 
via our <a href="https://access.cKnowledge.org">reproducibility challenges</a>
It allows the authors to communicate with the community and quickly fix encountered issues 
before submitting the final version to archival repositories.

 Other acceptable methods include:
 <ul>
  <li>
   Using zip or tar files with all related code and data, particularly when your artifact
   should be rebuilt on reviewers' machines (for example to have a non-virtualized access to a specific hardware).
  </li>
  <li>
   Using <a href="https://www.docker.com">Docker</a>, <a href="https://www.virtualbox.org">Virtual Box</a> and other containers and VM images.
  </li>
  <li>
   Arranging remote access to the authors' machine with the pre-installed software
   - this is an exceptional cases when rare or proprietary software and hardware is used.
   You will need to privately send the access information to the AE chairs.
  </li>
 </ul>

<p>
 Note that your artifacts will receive the ACM "artifact available" badge
 <b>only if</b> they have been placed on any publicly accessible archival repository
 such as <a href="https://zenodo.org">Zenodo</a>, <a href="https://figshare.com">FigShare</a>,
 and <a href="http://datadryad.org">Dryad</a>.
 You will need to provide a DOI assigned by those repositories in your final Artifact Appendix
 in the very end of the artifact evaluation process.

</div>

<!------------------------------------------------>
<div id="vspace8"></div>
<div id="vspace8"></div>
<h4>Artifact review (single-blind or public)</h4>

<div style="margin-left:20px;">

<p>
  Reviewers will need to read a paper and then thoroughly go through the
  Artifact Appendix step-by-step to evaluate a given artifact based on
  a set of <a href="https://github.com/mlcommons/ck/blob/master/docs/artifact-evaluation/reviewing.md">reviewing guidelines</a>.

<p>
  Reviewers are strongly encouraged to communicate with the authors about
  encountered issues immediately (and anonymously) via the HotCRP submission
  website to give the authors time to resolve all problems! Note that our
  philosophy of artifact evaluation is not to fail problematic artifacts but to
  help the authors improve their artifacts (at least publicly available ones)
  and pass the evaluation!

<p>
  In the end, AE chairs will decide on a set of the standard ACM reproducibility
  badges to award to a given artifact based on all reviews as well as the
  authors' responses.
</div>

<!------------------------------------------------>
<!--div id="vspace8"></div>
<div id="vspace8"></div>
<h4>ACM reproducibility badges</h4>

<div style="margin-left:20px;">



</div-->

<!------------------------------------------------>
<div id="vspace8"></div>
<div id="vspace8"></div>
<h4>Artifact Evaluation Committee</h4>

<div style="margin-left:20px;">
TBA
</div>



<!------------------------------------------------>
<div id="vspace8"></div>
<div id="vspace8"></div>
<h4>Questions and feedback</h4>

<div style="margin-left:20px;">
Please check the <a href="https://github.com/mlcommons/ck/blob/master/docs/artifact-evaluation/faq.md">AE FAQs</a> 
and feel free to ask questions or provide your feedback and suggestions
via our <a href="https://discord.gg/JjWNWXKxwT">Discord server</a> and/or the <a href="https://groups.google.com/forum/#!forum/artifact-evaluation">public AE discussion group</a>.
</div>

<div id="vspace8"></div>
<div id="vspace8"></div>
