<div id="vspace8"></div>

<center>
[ <b><a href="https://learning.acm.org/techtalks/reproducibility">ACM TechTalk about AE</a></b> ]
[ <a href="https://cTuning.org/ae">News</a> ]
</center>

<div id="vspace8"></div>

This document (<b>V20201122</b>) provides the guidelines to submit your artifacts for evaluation across a range of systems and machine learning conferences and journals.

<!-------------------------------------------------------------------------------------------->
<p>
<h2><a name="expect">Motivation</a></h2>

<div style="margin-left:20px;">
<p>
It's becoming increasingly difficult to <a href="https://www.reddit.com/r/MachineLearning/comments/ioq8do/n_reproducing_150_research_papers_the_problems">reproduce results from CS papers</a>. 
Voluntarily Artifact Evaluation (AE) was successfully introduced
at program languages, systems and machine learning conferences and tournaments 
to validate experimental results by the independent AE Committee, share unified Artifact Appendices, 
and assign reproducibility badges.

<p>
AE promotes the reproducibility of experimental results 
and encourages artifact sharing to help the community quickly validate and compare alternative approaches.
Authors are invited to formally describe all supporting material (code, data, models, workflows, results) 
using the <a href="$#ck_root_page_url#$checklist$#ck_page_suffix#$">unified Artifact Appendix and the Reproducibility Checklist template</a>
and submit it to the <a href="$#ck_root_page_url#$reviewing$#ck_page_suffix#$">single-blind AE process</a>.
Reviewers will then collaborate with the authors to evaluate their artifacts and assign the following
<a href="https://www.acm.org/publications/policies/artifact-review-and-badging-current">ACM reproducibility badges</a>:

 <center>
  <img src="https://www.acm.org/binaries/content/gallery/acm/publications/replication-badges/artifacts_available_dl.jpg" width="64">
  <img src="https://www.acm.org/binaries/content/gallery/acm/publications/replication-badges/artifacts_evaluated_functional_dl.jpg" width="64">
<!--  <img src="https://www.acm.org/binaries/content/gallery/acm/publications/replication-badges/artifacts_evaluated_reusable_dl.jpg" width="64"> -->
<!--  <img src="https://www.acm.org/binaries/content/gallery/acm/publications/replication-badges/results_replicated_dl.jpg" width="64"> -->
  <img src="https://www.acm.org/binaries/content/gallery/acm/publications/replication-badges/results_reproduced_dl.jpg" width="64">
 </center>

</div>

<!-------------------------------------------------------------------------------------------->
<p>
<h2><a name="prepare">Preparing your Artifact Appendix and the Reproducibility Checklist</a></h2>

<div style="margin-left:20px;">

<p>
 You need to prepare the <a href="https://github.com/ctuning/ck-artifact-evaluation/blob/master/wfe/artifact-evaluation/templates/ae.tex">Artifact Appendix</a>
 describing all software, hardware and data set dependencies, key results to be reproduced, and how to prepare, run and validated experiments.

 Though it is relatively intuitive and based on our 
 <a href="$#ck_root_page_url#$prior_ae$#ck_page_suffix#$">past AE experience and your feedback</a>, 
 we strongly encourage you to check the 
  the <a href="$#ck_root_page_url#$checklist$#ck_page_suffix#$">Artifact Appendix guide</a>,
  <a href="$#ck_root_page_url#$reviewing$#ck_page_suffix#$">artifact reviewing guide</a>,
  the <a href="https://www.sigplan.org/Resources/EmpiricalEvaluation">SIGPLAN Empirical Evaluation Guidelines</a>,
  the <a href="https://www.cs.mcgill.ca/~jpineau/ReproducibilityChecklist.pdf">NeurIPS reproducibility checklist</a>
  and <a href="$#ck_root_page_url#$faq$#ck_page_suffix#$">AE FAQs</a> before submitting artifacts for evaluation!

 You can find the examples of Artifact Appendices 
 in the following <a href="https://cKnowledge.io/reproduced-papers">reproduced papers</a>.

 <p>
 <i>Since the AE methodology is slightly different at different conferences, we introduced the unified Artifact Appendix 
 with the Reproducibility Checklist to help readers understand what was evaluated and how! Furthermore, artifact evaluation 
 sometimes help to discover some minor mistakes in the accepted paper -
 in such case you have a chance to add related notes and corrections
 in the Artifact Appendix of your camera-ready paper!</i>

</div>

<!-------------------------------------------------------------------------------------------->
<p>
<h2><a name="prepare">Preparing your experimental workflow</a></h2>

<div style="margin-left:20px;">

 <p><b>You can skip this step if you want to share your artifacts without the validation of experimental results - 
 in such case your paper can still be entitled for the "artifact available" badge!</b>

 <p>
 We strongly recommend you to provide at least some scripts to build your workflow, 
 all inputs to run your workflow, and some expected outputs to validate results from your paper.
 You can then describe the steps to evaluate your artifact 
 using <a href="https://jupyter.org">Jupyter Notebooks</a>
 or plain ReadMe files.


</div>

<!-------------------------------------------------------------------------------------------->
<p>
<h2><a name="prepare">Making artifacts available to evaluators</a></h2>

<div style="margin-left:20px;">

Most of the time, the authors make their artifacts available to the evaluators via GitHub,
GitLab, BitBucket or similar private or public service. Public artifact sharing allows
optional "open evaluation" which we have successfully validated at <a href="https://adapt-workshop.org">ADAPT'16</a>
and <a href="https://cKnowledge.org/request">ASPLOS-REQUEST'18</a>.
It allows the authors to quickly fix encountered issues during evaluation
before submitting the final version to archival repositories.

<p>
Other acceptable methods include:
 <ul>
  <li>
   Using zip or tar files with all related code and data, particularly when your artifact
   should be rebuilt on reviewers' machines (for example to have a non-virtualized access to a specific hardware). 
  </li>
  <li>
   Using <a href="https://www.docker.com">Docker</a>, <a href="https://www.virtualbox.org">Virtual Box</a> and other containers and VM images. 
  </li>
  <li>
   Arranging remote access to the authors' machine with the pre-installed software 
   - this is an exceptional cases when rare or proprietary software and hardware is used.
   You will need to privately send the access information to the AE chairs.
  </li>
 </ul>

<p>
Note that your artifacts will receive the ACM "artifact available" badge
<b>only if</b> they have been placed on any publicly accessible archival repository
such as <a href="https://zenodo.org">Zenodo</a>, <a href="https://figshare.com">FigShare</a>,
and <a href="http://datadryad.org">Dryad</a>. 
You must provide a DOI automatically assigned to your artifact by these repositories 
in your final Artifact Appendix!

</div>

<!-------------------------------------------------------------------------------------------->
<p>
<h2><a name="prepare">Submitting artifacts</a></h2>

<div style="margin-left:20px;">

 Write a brief abstract describing your artifact, the minimal hardware and software requirements, 
 how it supports your paper, how it can be validated and what the expected result is. 
 Do not forget to specify if you use any proprietary software or hardware!
 This abstract will be used by evaluators during artifact bidding to make sure that
 they have an access to appropriate hardware and software and have required skills.

 <p>
 Submit the artifact abstract and the PDF of your paper with the Artifact Appendix attached 
 using the AE submission website provided by the event.

</div>

<!-------------------------------------------------------------------------------------------->
<p>
<h2><a name="prepare">Asking questions</a></h2>

<div style="margin-left:20px;">
 If you have questions or suggestions, 
 do not hesitate to get in touch with the the AE chairs or the community using 
 the <a href="https://groups.google.com/forum/#!forum/artifact-evaluation">Artifact Evaluation google group</a>,
</div>

<!-------------------------------------------------------------------------------------------->
<p>
<h2><a name="accepted">Preparing your camera-ready paper</a></h2>

<div style="margin-left:20px;">

 If you have successfully passed AE with at least one reproducibility badge, 
 you will need to add up to 2 pages of your artifact appendix 
 to your camera ready paper while removing all unnecessary or confidential information. 
 This will help readers better understand what was evaluated and how.
 <p>

 If your paper is published in the ACM Digital Library,
 you do not need to add reproducibility stamps - ACM will add them to your camera-ready paper
 and will make this information available for search!
 In other cases, AE chairs will tell you how to add stamps to the first page of your paper.

</div>


<!-------------------------------------------------------------------------------------------->
<p>
<h2><a name="examples">Examples of reproduced papers with shared artifacts and Artifact Appendices:</h2>

<div style="margin-left:20px;">

  <ul>
   <li>
    <a href="https://cKnowledge.io/?q=%22reproduced-papers%22">Some papers from the past AE</a> (ASPLOS, MLSys, Supercomputing, CGO, PPoPP, PACT, IA3, ReQuEST)
   </li>
   <li>
    <a href="https://cKnowledge.io/?q=%22reproduced-papers%22%20AND%20%22portable-workflow-ck%22">Papers with automated and portable workflows</a>
   </li>
   <li>
    <a href="https://cknowledge.io/c/report/rpi3-crowd-tuning-2017-interactive/">"Live" papers</a> (collaborative validation by the community)
   </li>
   <li>
    <a href="https://cKnowledge.io/?q=%22reproduced-results%22">Dashboards with reproduced results</a>
   </li>
   <li>
    Paper "Highly Efficient 8-bit Low Precision Inference of Convolutional Neural Networks with IntelCaffe" from ACM ASPLOS-ReQuEST'18<br>
    <div style="margin-left:20px;">
     <i><a href="https://doi.org/10.1145/3229762.3229763">Paper DOI</a>,  <a href="https://doi.org/10.1145/3229769">Artifact DOI</a>, 
     <a href="https://github.com/intel/caffe/wiki/ReQuEST-Artifact-Installation-Guide">Original artifact</a>,
     <a href="https://github.com/ctuning/ck-request-asplos18-caffe-intel">portable workflow</a>,
     <a href="https://github.com/ctuning/ck-request-asplos18-results-caffe-intel">Expected results</a>,
     <a href="https://cKnowledge.io/result/pareto-efficient-ai-co-design-tournament-request-acm-asplos-2018/">Public scoreboard</a></i>
    </div>
   </li>
   <li>
    Paper "Software Prefetching for Indirect Memory Accesses" from CGO'17<br>
    <div style="margin-left:20px;">
     <i><a href="https://github.com/SamAinsworth/reproduce-cgo2017-paper">Portable workflow at GitHub</a>,
     <a href="$#ck_root_page_url#$resources/paper-with-distinguished-ck-artifact-and-ae-appendix-cgo2017.pdf">PDF with the Artifact Appendix</a>,
     <a href="https://github.com/SamAinsworth/reproduce-cgo2017-paper/files/618737/ck-aarch64-dashboard.pdf">CK dashboard snapshot</a></i>
    </div>
   </li>
  </ul>

</div>

<!-------------------------------------------------------------------------------------------->
<p>
<h2><a name="archive">Methodology archive</a></h2>

<div style="margin-left:20px;">

  List of different versions of our artifact submission and reviewing guides
  to help you understand which one was used in papers with evaluated artifacts:

  <ul>
   <li><b>V20200102 (MLSys'20)</b>: 
    <a href="$#ck_url_template_pull#$templates/ae-20200102.tex">LaTeX template</a>,
    <a href="$#ck_root_page_url#$submission-20200102$#ck_page_suffix#$">submission guide</a>,
    <a href="$#ck_root_page_url#$reviewing-20200102$#ck_page_suffix#$">reviewing guide</a>
   <li><b>V20190109 (ASPLOS'20/MLSys'19)</b>: 
    <a href="$#ck_url_template_pull#$templates/ae-20190108.tex">LaTeX template</a>,
    <a href="$#ck_root_page_url#$submission-20190109$#ck_page_suffix#$">submission guide</a>,
    <a href="$#ck_root_page_url#$reviewing-20190109$#ck_page_suffix#$">reviewing guide</a>
   <li><b>V20180713 (CGO'19/PPoPP'19/PACT'18/IA3'18/ReQuEST'18)</b>: 
    <a href="$#ck_url_template_pull#$templates/ae-20180713.tex">LaTeX template</a>,
    <a href="$#ck_root_page_url#$submission-20180713$#ck_page_suffix#$">submission guide</a>,
    <a href="$#ck_root_page_url#$reviewing-20180713$#ck_page_suffix#$">reviewing guide</a>
   <li><b>V20171101 (CGO'18/PPoPP'18)</b>: 
    <a href="$#ck_url_template_pull#$templates/ae-20170622.tex">LaTeX template</a>,
    <a href="$#ck_root_page_url#$submission-20171101$#ck_page_suffix#$">submission guide</a>,
    <a href="$#ck_root_page_url#$reviewing-20171101$#ck_page_suffix#$">reviewing guide</a>
   <li><b>V20170414 (PACT'17)</b>: 
    <a href="$#ck_url_template_pull#$templates/ae-20160509.tex">LaTeX template</a>,
    <a href="$#ck_root_page_url#$submission-20170414$#ck_page_suffix#$">submission guide</a>,
    <a href="$#ck_root_page_url#$reviewing-20170414$#ck_page_suffix#$">reviewing guide</a>
   <li><b>V20161020 (PPoPP'17/CGO'17)</b>: 
    <a href="$#ck_url_template_pull#$templates/ae-20160509.tex">LaTeX template</a>,
    <a href="$#ck_root_page_url#$submission-20161020$#ck_page_suffix#$">submission guide</a>,
    <a href="$#ck_root_page_url#$reviewing-20161020$#ck_page_suffix#$">reviewing guide</a>
   <li><b>V20160509 (PACT'16)</b>: 
    <a href="$#ck_url_template_pull#$templates/ae-20160509.tex">LaTeX template</a>,
    <a href="$#ck_root_page_url#$submission-20160509$#ck_page_suffix#$">submission guide</a>,
    <a href="$#ck_root_page_url#$reviewing-20160509$#ck_page_suffix#$">reviewing guide</a>
   <li><b>V20151015 (PPoPP'16/CGO'16/ADAPT'16)</b>: 
    <a href="$#ck_url_template_pull#$templates/ae-20151015.tex">LaTeX template</a>,
    <a href="$#ck_root_page_url#$submission-20151015$#ck_page_suffix#$">submission guide</a>,
    <a href="$#ck_root_page_url#$reviewing-20151015$#ck_page_suffix#$">reviewing guide</a>
  </ul>

</div>

<hr>
  We regularly update this document
  based on public <a href="https://www.reddit.com/r/MachineLearning/comments/ioq8do/n_reproducing_150_research_papers_the_problems">discussions</a>,
  <a href="prior_ae.html">past Artifact Evaluation experience</a>,
  <a href="https://docs.google.com/document/d/1QZzRVMZMsMev3lxBgHG4TFHEFXy__N7B69ysWXi0ZcY/edit">the feedback from researchers</a>,
  <a href="http://artifact-eval.org">PL AE</a>, 
  and the <a href="https://www.acm.org/publications/policies/artifact-review-and-badging-current">ACM reviewing and badging policy</a>.
  Our goal is to come up with a common methodology, <a href="submission_extra.html">unified artifact appendix with the reproducibility checklist</a>
  and an <a href="https://cKnowledge.io">open reproducibility platform</a> for artifact sharing, validation and reuse.
  If you have questions or suggestions, do not hesitate to get in touch with the AE community using 
  the <a href="https://groups.google.com/forum/#!forum/artifact-evaluation">dedicated AE google group</a>.


<br>
<br>
